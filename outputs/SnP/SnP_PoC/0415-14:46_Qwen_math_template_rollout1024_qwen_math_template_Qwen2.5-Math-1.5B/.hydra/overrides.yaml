- trainer.user_name=SnP
- trainer.group_name=SnP_PoC
- trainer.experiment_name=0415-14:46_Qwen_math_template_rollout1024_qwen_math_template_Qwen2.5-Math-1.5B
- trainer.logger=['tensorboard']
- trainer.n_gpus_per_node=4
- trainer.nnodes=1
- trainer.save_freq=-1
- trainer.test_freq=10000
- trainer.total_training_steps=1000
- trainer.resume_from_path=False
- trainer.default_local_dir=outputs/SnP/SnP_PoC/0415-14:46_Qwen_math_template_rollout1024_qwen_math_template_Qwen2.5-Math-1.5B/checkpoint
- trainer.default_hdfs_dir=outputs/SnP/SnP_PoC/0415-14:46_Qwen_math_template_rollout1024_qwen_math_template_Qwen2.5-Math-1.5B/checkpoint
- trainer.sp_freq=10000000
- trainer.sp_alpha=1.0
- data.train_files=data/understand_r1_math_12k/qwen_math_template/train.parquet
- data.val_files=data/understand_r1_math_12k/qwen_math_template/train.parquet
- data.train_batch_size=1024
- data.max_prompt_length=1024
- data.max_response_length=2048
- actor_rollout_ref.model.path=Qwen/Qwen2.5-Math-1.5B
- actor_rollout_ref.actor.optim.lr=1e-6
- actor_rollout_ref.actor.epochs=1
- actor_rollout_ref.actor.mini_batch_size=128
- actor_rollout_ref.actor.micro_batch_size_per_gpu=2
- actor_rollout_ref.actor.use_kl_loss=True
- actor_rollout_ref.actor.kl_loss_coef=0.0001
- actor_rollout_ref.actor.kl_loss_type=low_var_kl
- actor_rollout_ref.actor.entropy_coeff=0.0
- actor_rollout_ref.actor.fsdp_config.param_offload=False
- actor_rollout_ref.actor.fsdp_config.optimizer_offload=False
- actor_rollout_ref.actor.target_entropy_loss.enable=False
- actor_rollout_ref.actor.target_entropy_loss.coefficient=0.001
- actor_rollout_ref.actor.target_entropy_loss.target_value=0.5
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.rollout.gpu_memory_utilization=0.4
- actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8
- actor_rollout_ref.rollout.tensor_model_parallel_size=1
- actor_rollout_ref.rollout.n=8
- actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8
- actor_rollout_ref.ref.fsdp_config.param_offload=True
- reward_model.format_reward=0.0
- reward_model.answer_reward=1.0
- algorithm.gamma=1.0
