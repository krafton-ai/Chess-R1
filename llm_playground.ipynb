{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb65319b0a442c388d3282b8d5448ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set up model parameters\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# max_tokens = 2048     # Maximum tokens to generate in response\n",
    "# temperature = 0.6    # Sampling temperature\n",
    "# top_p = 0.9\n",
    "# top_k = 50\n",
    "# repetition_penalty = 1.0\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"  \n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "max_tokens = 2048     # Maximum tokens to generate in response\n",
    "temperature = 1.0    # Sampling temperature\n",
    "\n",
    "\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant who plays chess professionally.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "This is something related to chess.\n",
      "What do you think this is?\n",
      "\n",
      "a8: r b8: - c8: - d8: - e8: - f8: - g8: - h8: k a7: p b7: p c7: - d7: - e7: r f7: - g7: - h7: p a6: - b6: - c6: - d6: - e6: R f6: p g6: - h6: Q a5: - b5: - c5: - d5: p e5: - f5: - g5: - h5: - a4: - b4: - c4: - d4: - e4: - f4: - g4: - h4: - a3: - b3: N c3: - d3: P e3: - f3: - g3: b h3: - a2: P b2: q c2: P d2: - e2: - f2: - g2: P h2: P a1: - b1: - c1: - d1: - e1: - f1: - g1: - h1: K\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### system prompts\n",
    "\n",
    "system_prompt = \"\"\"\\\n",
    "You are a helpful assistant who plays chess professionally.\n",
    "The assistant first thinks through the reasoning process internally and then provides the user with the best move.\n",
    "The reasoning process and the answer must be enclosed within <think> </think> and <answer> </answer> tags, respectively.\n",
    "The reasoning process should describe how you analyze the position and decide on the best move.\n",
    "The answer must be in SAN notation, strictly using the moving piece and the destination square (e.g., Nf3, Rxf2, c5). \n",
    "Now, the user provides a FEN string and a list of legal moves for the given board.\n",
    "After analyzing the position, clearly state the best move in SAN notation within <answer> </answer> tags. i.e., <answer> Nf3 </answer>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"\\\n",
    "You are a helpful assistant who plays chess professionally.\n",
    "\"\"\"\n",
    "\n",
    "#### user prompts\n",
    "\n",
    "user_prompt = \"\"\"\\\n",
    "Previous moves: 1. e4 c6 2. d4 d5 3. Nc3 dxe4 4. Nxe4 Nf6 5. Nxf6+ exf6 6. Nf3 Bd6 7. Bd3 O-O 8. O-O Bg4 9. c3 Nd7 10. h3 Bh5 11. Re1 Qc7 12. Be2 Rfe8 13. Nd2 Bxe2 14. Rxe2 f5 15. Nc4 Nf6 16. Bg5 Bh2+ 17. Kh1 Ne4 18. Bd2 Nxd2 19. Qxd2 Bf4 20. Rae1 Kf8 21. Qd3 g6 22. g3 Bg5 23. h4 Bf6 24. Kg2 Rxe2 25. Rxe2 Rd8 26. Qe3 h5 27. Ne5 Kg7 28. Nd3 b6 29. Qf4 Qd7 30. Ne5 Qd5+ 31. Qf3 Bxe5 32. Qxd5 Rxd5 33. Rxe5 Rxe5 34. dxe5 f6 35. f4 fxe5\\nCurrent FEN string: 8/p5k1/1pp3p1/4pp1p/5P1P/2P3P1/PP4K1/8 w - - 0 36\\nLegal moves: Kh3 Kf3 Kh2 Kf2 Kh1 Kg1 Kf1 fxe5 g4 c4 b3 a3 b4 a4\\n\n",
    "\"\"\"\n",
    "\n",
    "qa_user_prompt = \"\"\"\\\n",
    "This is something related to chess.\n",
    "What do you think this is?\n",
    "\n",
    "a8: r b8: - c8: - d8: - e8: - f8: - g8: - h8: k a7: p b7: p c7: - d7: - e7: r f7: - g7: - h7: p a6: - b6: - c6: - d6: - e6: R f6: p g6: - h6: Q a5: - b5: - c5: - d5: p e5: - f5: - g5: - h5: - a4: - b4: - c4: - d4: - e4: - f4: - g4: - h4: - a3: - b3: N c3: - d3: P e3: - f3: - g3: b h3: - a2: P b2: q c2: P d2: - e2: - f2: - g2: P h2: P a1: - b1: - c1: - d1: - e1: - f1: - g1: - h1: K\n",
    "\"\"\"\n",
    "\n",
    "assistant_prompt = \"\"\"\\\n",
    "Let's think step by step. <think>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define a sample conversation (list of messages)\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": qa_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": qa_user_prompt},\n",
    "]\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation, \n",
    "            tokenize=True, \n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\").to(device)\n",
    "\n",
    "# concat assistant related prompts\n",
    "# inputs = torch.cat([inputs, tokenizer(assistant_prompt, return_tensors=\"pt\").to(device)['input_ids']], dim=-1)\n",
    "print(tokenizer.decode(inputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant who plays chess professionally.\n",
      "\n",
      "user\n",
      "This is something related to chess.\n",
      "What do you think this is?\n",
      "\n",
      "a8: r b8: - c8: - d8: - e8: - f8: - g8: - h8: k a7: p b7: p c7: - d7: - e7: r f7: - g7: - h7: p a6: - b6: - c6: - d6: - e6: R f6: p g6: - h6: Q a5: - b5: - c5: - d5: p e5: - f5: - g5: - h5: - a4: - b4: - c4: - d4: - e4: - f4: - g4: - h4: - a3: - b3: N c3: - d3: P e3: - f3: - g3: b h3: - a2: P b2: q c2: P d2: - e2: - f2: - g2: P h2: P a1: - b1: - c1: - d1: - e1: - f1: - g1: - h1: K\n",
      "\n",
      "assistant\n",
      "The board you've described appears to be a starting position for a chess game with some pieces missing or mispositioned. Let's analyze the setup step-by-step:\n",
      "\n",
      "```\n",
      " a8 r  b8 -  c8 -  d8 -  e8 -  f8 -  g8 -  h8 k\n",
      " a7 p  b7 -  c7 -  d7 -  e7 r  f7 -  g7 -  h7 p\n",
      " a6 -  b6 -  c6 -  d6 -  e6 R  f6 p  g6 -  h6 Q\n",
      " a5 -  b5 -  c5 -  d5 p  e5 -  f5 -  g5 -  h5 -\n",
      " a4 -  b4 -  c4 -  d4 -  e4 -  f4 -  g4 -  h4 -\n",
      " a3 -  b3 N  c3 -  d3 P  e3 -  f3 -  g3 b  h3 -\n",
      " a2 p  b2 q  c2 P  d2 -  e2 -  f2 -  g2 P  h2 P\n",
      " a1 -  b1 -  c1 -  d1 -  e1 -  f1 -  g1 -  h1 K\n",
      "```\n",
      "\n",
      "### Analysis:\n",
      "- **King (K)** is on `h1` instead of `e1`.\n",
      "- **Queen (Q)** is on `h8` instead of `d8`.\n",
      "- **Rook (R)** is on `f8` instead of `h1`.\n",
      "- **Knight (N)** is on `b3` instead of `g1`.\n",
      "- **Bishops** are missing from both sides.\n",
      "- **White Pawns** are missing from `c2`, `d2`, and `f2`.\n",
      "\n",
      "### Correct Setup:\n",
      "The standard starting position should look like this:\n",
      "\n",
      "```\n",
      " a8 r  b8 n  c8 b  d8 q  e8 k  f8 b  g8 n  h8 r \n",
      " a7 p  b7 p  c7 p  d7 p  e7 p  f7 p  g7 p  h7 p \n",
      " a6 -  b6 -  c6 -  d6 -  e6 -  f6 -  g6 -  h6 - \n",
      " a5 -  b5 -  c5 -  d5 -  e5 -  f5 -  g5 -  h5 - \n",
      " a4 -  b4 -  c4 -  d4 -  e4 -  f4 -  g4 -  h4 - \n",
      " a3 -  b3 -  c3 -  d3 -  e3 -  f3 -  g3 -  h3 - \n",
      " a2 P  b2 P  c2 P  d2 P  e2 P  f2 P  g2 P  h2 P \n",
      " a1 R  b1 N  c1 B  d1 Q  e1 K  f1 B  g1 N  h1 r \n",
      "```\n",
      "\n",
      "### Conclusion:\n",
      "The provided board has several misplacements and missing pieces, which would significantly alter the gameplay and make it invalid for a proper chess game. If this is a mistake in a puzzle or a game, it needs to be corrected to the standard starting position.\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(full_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a8: r b8: - c8: - d8: - e8: - f8: - g8: - h8: k\n",
      "a7: p b7: p c7: - d7: - e7: r f7: - g7: - h7: p\n",
      "a6: - b6: - c6: - d6: - e6: R f6: p g6: - h6: Q\n",
      "a5: - b5: - c5: - d5: p e5: - f5: - g5: - h5: -\n",
      "a4: - b4: - c4: - d4: - e4: - f4: - g4: - h4: -\n",
      "a3: - b3: N c3: - d3: P e3: - f3: - g3: b h3: -\n",
      "a2: P b2: q c2: P d2: - e2: - f2: - g2: P h2: P\n",
      "a1: - b1: - c1: - d1: - e1: - f1: - g1: - h1: K\n"
     ]
    }
   ],
   "source": [
    "print(\"a8: r b8: - c8: - d8: - e8: - f8: - g8: - h8: k\\na7: p b7: p c7: - d7: - e7: r f7: - g7: - h7: p\\na6: - b6: - c6: - d6: - e6: R f6: p g6: - h6: Q\\na5: - b5: - c5: - d5: p e5: - f5: - g5: - h5: -\\na4: - b4: - c4: - d4: - e4: - f4: - g4: - h4: -\\na3: - b3: N c3: - d3: P e3: - f3: - g3: b h3: -\\na2: P b2: q c2: P d2: - e2: - f2: - g2: P h2: P\\na1: - b1: - c1: - d1: - e1: - f1: - g1: - h1: K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
