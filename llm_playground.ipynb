{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f61866dcd064043b1f6462be7d6dd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set up model parameters\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# max_tokens = 2048     # Maximum tokens to generate in response\n",
    "# temperature = 0.6    # Sampling temperature\n",
    "# top_p = 0.9\n",
    "# top_k = 50\n",
    "# repetition_penalty = 1.0\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"  \n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "max_tokens = 2048     # Maximum tokens to generate in response\n",
    "temperature = 1.0    # Sampling temperature\n",
    "\n",
    "\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a helpful assistant who plays chess professionally.\\nThe assistant first thinks through the reasoning process internally and then provides the user with the best move.\\nThe reasoning process and the answer must be enclosed within <think> </think> and <answer> </answer> tags, respectively.\\nThe reasoning process should describe how you analyze the position and decide on the best move.\\nThe answer must be in SAN notation, strictly using the moving piece and the destination square (e.g., Nf3, Rxf2, c5). \\nNow, the user provides a FEN string and a list of legal moves for the given board.\\nAfter analyzing the position, clearly state the best move in SAN notation within <answer> </answer> tags. i.e., <answer> Nf3 </answer>\\n<|im_end|>\\n<|im_start|>user\\nPrevious moves: 1. e4 c6 2. d4 d5 3. Nc3 dxe4 4. Nxe4 Nf6 5. Nxf6+ exf6 6. Nf3 Bd6 7. Bd3 O-O 8. O-O Bg4 9. c3 Nd7 10. h3 Bh5 11. Re1 Qc7 12. Be2 Rfe8 13. Nd2 Bxe2 14. Rxe2 f5 15. Nc4 Nf6 16. Bg5 Bh2+ 17. Kh1 Ne4 18. Bd2 Nxd2 19. Qxd2 Bf4 20. Rae1 Kf8 21. Qd3 g6 22. g3 Bg5 23. h4 Bf6 24. Kg2 Rxe2 25. Rxe2 Rd8 26. Qe3 h5 27. Ne5 Kg7 28. Nd3 b6 29. Qf4 Qd7 30. Ne5 Qd5+ 31. Qf3 Bxe5 32. Qxd5 Rxd5 33. Rxe5 Rxe5 34. dxe5 f6 35. f4 fxe5\\nCurrent FEN string: 8/p5k1/1pp3p1/4pp1p/5P1P/2P3P1/PP4K1/8 w - - 0 36\\nLegal moves: Kh3 Kf3 Kh2 Kf2 Kh1 Kg1 Kf1 fxe5 g4 c4 b3 a3 b4 a4\\n\\n<|im_end|>\\n<|im_start|>assistant\\nLet's think step by step. <think>\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#### system prompts\n",
    "\n",
    "system_prompt = \"\"\"\\\n",
    "You are a helpful assistant who plays chess professionally.\n",
    "The assistant first thinks through the reasoning process internally and then provides the user with the best move.\n",
    "The reasoning process and the answer must be enclosed within <think> </think> and <answer> </answer> tags, respectively.\n",
    "The reasoning process should describe how you analyze the position and decide on the best move.\n",
    "The answer must be in SAN notation, strictly using the moving piece and the destination square (e.g., Nf3, Rxf2, c5). \n",
    "Now, the user provides a FEN string and a list of legal moves for the given board.\n",
    "After analyzing the position, clearly state the best move in SAN notation within <answer> </answer> tags. i.e., <answer> Nf3 </answer>\n",
    "\"\"\"\n",
    "\n",
    "#### user prompts\n",
    "\n",
    "user_prompt = \"\"\"\\\n",
    "Previous moves: 1. e4 c6 2. d4 d5 3. Nc3 dxe4 4. Nxe4 Nf6 5. Nxf6+ exf6 6. Nf3 Bd6 7. Bd3 O-O 8. O-O Bg4 9. c3 Nd7 10. h3 Bh5 11. Re1 Qc7 12. Be2 Rfe8 13. Nd2 Bxe2 14. Rxe2 f5 15. Nc4 Nf6 16. Bg5 Bh2+ 17. Kh1 Ne4 18. Bd2 Nxd2 19. Qxd2 Bf4 20. Rae1 Kf8 21. Qd3 g6 22. g3 Bg5 23. h4 Bf6 24. Kg2 Rxe2 25. Rxe2 Rd8 26. Qe3 h5 27. Ne5 Kg7 28. Nd3 b6 29. Qf4 Qd7 30. Ne5 Qd5+ 31. Qf3 Bxe5 32. Qxd5 Rxd5 33. Rxe5 Rxe5 34. dxe5 f6 35. f4 fxe5\\nCurrent FEN string: 8/p5k1/1pp3p1/4pp1p/5P1P/2P3P1/PP4K1/8 w - - 0 36\\nLegal moves: Kh3 Kf3 Kh2 Kf2 Kh1 Kg1 Kf1 fxe5 g4 c4 b3 a3 b4 a4\\n\n",
    "\"\"\"\n",
    "\n",
    "assistant_prompt = \"\"\"\\\n",
    "Let's think step by step. <think>\n",
    "\"\"\"\n",
    "\n",
    "# Define a sample conversation (list of messages)\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation, \n",
    "            tokenize=True, \n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\").to(device)\n",
    "\n",
    "# concat assistant related prompts\n",
    "inputs = torch.cat([inputs, tokenizer(assistant_prompt, return_tensors=\"pt\").to(device)['input_ids']], dim=-1)\n",
    "tokenizer.decode(inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant who plays chess professionally.\n",
      "The assistant first thinks through the reasoning process internally and then provides the user with the best move.\n",
      "The reasoning process and the answer must be enclosed within <think> </think> and <answer> </answer> tags, respectively.\n",
      "The reasoning process should describe how you analyze the position and decide on the best move.\n",
      "The answer must be in SAN notation, strictly using the moving piece and the destination square (e.g., Nf3, Rxf2, c5). \n",
      "Now, the user provides a FEN string and a list of legal moves for the given board.\n",
      "After analyzing the position, clearly state the best move in SAN notation within <answer> </answer> tags. i.e., <answer> Nf3 </answer>\n",
      "\n",
      "user\n",
      "Previous moves: 1. e4 c6 2. d4 d5 3. Nc3 dxe4 4. Nxe4 Nf6 5. Nxf6+ exf6 6. Nf3 Bd6 7. Bd3 O-O 8. O-O Bg4 9. c3 Nd7 10. h3 Bh5 11. Re1 Qc7 12. Be2 Rfe8 13. Nd2 Bxe2 14. Rxe2 f5 15. Nc4 Nf6 16. Bg5 Bh2+ 17. Kh1 Ne4 18. Bd2 Nxd2 19. Qxd2 Bf4 20. Rae1 Kf8 21. Qd3 g6 22. g3 Bg5 23. h4 Bf6 24. Kg2 Rxe2 25. Rxe2 Rd8 26. Qe3 h5 27. Ne5 Kg7 28. Nd3 b6 29. Qf4 Qd7 30. Ne5 Qd5+ 31. Qf3 Bxe5 32. Qxd5 Rxd5 33. Rxe5 Rxe5 34. dxe5 f6 35. f4 fxe5\n",
      "Current FEN string: 8/p5k1/1pp3p1/4pp1p/5P1P/2P3P1/PP4K1/8 w - - 0 36\n",
      "Legal moves: Kh3 Kf3 Kh2 Kf2 Kh1 Kg1 Kf1 fxe5 g4 c4 b3 a3 b4 a4\n",
      "\n",
      "\n",
      "assistant\n",
      "Let's think step by step. <think>\n",
      "1. The current position is after Black's 35th move, so it's White's turn to play.\n",
      "2. The king is exposed, and there are no immediate threats to it from Black.\n",
      "3. White has several pawn options to consider, but they all aim to control key squares or prepare for a potential queen move.\n",
      "4. The most strategic pawn move seems to be g4, which challenges the f5 pawn and prepares to advance further if needed.\n",
      "5. Alternatively, advancing pawns to b4 or c4 could also be considered to create more space and pressure on Black's position.\n",
      "\n",
      "Among these options, g4 is the most active and strategic move. It supports the g-pawn and can lead to a more open position where White's bishop on g5 can have more influence.\n",
      "\n",
      "Therefore, the best move is g4.\n",
      "</think>\n",
      "<answer>g4</answer>\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(full_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_fastlang\n",
    "import fasttext\n",
    "# Override fastText's internal warning function to a no-op.\n",
    "fasttext.FastText.eprint = lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
